{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseCompositeDocumentVectors:\n",
    "    def __init__(self, num_clusters,  pname1, pname2):\n",
    "        self.min_no = 0\n",
    "        self.max_no = 0\n",
    "        self.prob_wordvecs = {}\n",
    "        \n",
    "        #### 読み込むファイルの設定\n",
    "        # GloVeの単語ベクトルファイル\n",
    "        self.glove_word_vector_file = \"../elasticsearch/es_glove_vectors.txt\"\n",
    "        #self.glove_word_vector_file = \"../glove/glove_vectors.txt\"\n",
    "\n",
    "        #### 出力するファイルの設定\n",
    "        # GloVeの単語ベクトルに単語数とベクトルサイズを付与したファイル\n",
    "        self.gensim_glove_word_vector_file = \"../elasticsearch/es_gensim_glove_vectors.txt\"\n",
    "        #self.gensim_glove_word_vector_file = \"../glove/gensim_glove_vectors.txt\"\n",
    "        \n",
    "        # GMMの結果を保存するPickleファイル\n",
    "        self.pname1 = pname1\n",
    "        self.pname2 = pname2\n",
    "\n",
    "        #### その他パラメータ\n",
    "        # GMMのクラスタ数\n",
    "        self.num_clusters = num_clusters\n",
    "        \n",
    "        # GloVeの次元数\n",
    "        self.num_features = 50\n",
    "    \n",
    "    def load_glove_vector(self):\n",
    "        # GloVeの単語ベクトルファイルを読み込み、単語数とベクトルサイズを付与した処理用のファイルを作成する。\n",
    "        vectors = pd.read_csv(self.glove_word_vector_file, delimiter=' ', index_col=0, header=None)\n",
    "        \n",
    "        vocab_count = vectors.shape[0]  # 単語数\n",
    "        self.num_features = vectors.shape[1]  # 次元数\n",
    "\n",
    "        with open(self.glove_word_vector_file, 'r') as original, open(self.gensim_glove_word_vector_file, 'w') as transformed:\n",
    "            transformed.write(f'{vocab_count} {self.num_features}\\n')\n",
    "            transformed.write(original.read())  # 2行目以降はそのまま出力\n",
    "\n",
    "        # GloVeの単語ベクトルを読み込む        \n",
    "        self.glove_vectors = KeyedVectors.load_word2vec_format(self.gensim_glove_word_vector_file, binary=False)\n",
    "        \n",
    "    def cluster_GMM2(self):   \n",
    "        glove_vectors = self.glove_vectors.vectors\n",
    "        \n",
    "        # Initalize a GMM object and use it for clustering.\n",
    "        gmm_model = GaussianMixture(n_components=num_clusters, covariance_type=\"tied\", init_params='kmeans', max_iter=100)\n",
    "        # Get cluster assignments.\n",
    "        gmm_model.fit(glove_vectors)\n",
    "        idx = gmm_model.predict(glove_vectors)\n",
    "        print (\"Clustering Done...\")\n",
    "        # Get probabilities of cluster assignments.\n",
    "        idx_proba = gmm_model.predict_proba(glove_vectors)\n",
    "        # Dump cluster assignments and probability of cluster assignments. \n",
    "        pickle.dump(idx, open(self.pname1,\"wb\"))\n",
    "        print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "        pickle.dump(idx_proba,open(self.pname2, \"wb\"))\n",
    "        print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "        return (idx, idx_proba)        \n",
    "        \n",
    "    def cluster_GMM(self):\n",
    "        # GMMによるクラスタリング\n",
    "        \n",
    "        clf = GaussianMixture(\n",
    "            n_components=self.num_clusters,\n",
    "            #covariance_type=\"tied\",\n",
    "            covariance_type=\"diag\",            \n",
    "            init_params=\"kmeans\",\n",
    "            max_iter=50\n",
    "        )\n",
    "        \n",
    "        glove_vectors = self.glove_vectors.vectors\n",
    "        # Get cluster assignments.\n",
    "        clf.fit(glove_vectors)\n",
    "        idx = clf.predict(glove_vectors)\n",
    "        print(\"Clustering Done...\")\n",
    "        # Get probabilities of cluster assignments.\n",
    "        idx_proba = clf.predict_proba(glove_vectors)\n",
    "        # Dump cluster assignments and probability of cluster assignments.\n",
    "        pickle.dump(idx, open(self.pname1, \"wb\"))\n",
    "        print(\"Cluster Assignments Saved...\")\n",
    "        pickle.dump(idx_proba, open(self.pname2, \"wb\"))\n",
    "        print(\"Probabilities of Cluster Assignments saved...\")\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    def read_GMM(self):\n",
    "        # GMMモデルを読み込む。\n",
    "        \n",
    "        idx = pickle.load(open(self.idx_name, \"rb\"))\n",
    "        idx_proba = pickle.load(open(self.idx_proba_name, \"rb\"))\n",
    "        print(\"Cluster Model Loaded...\")\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    def get_idf_dict(self, corpus):\n",
    "        # IDFを算出する。\n",
    "        # corpus : 分かち書きした文章のリスト\n",
    "        \n",
    "        # 単語の数をカウントする\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        X_count = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # scikit-learn の TF-IDF 実装\n",
    "        tfidf_vectorizer = TfidfVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        feature_names = tfidf_vectorizer.get_feature_names()\n",
    "        idf = tfidf_vectorizer.idf_\n",
    "\n",
    "        word_idf_dict = {}\n",
    "        for pair in zip(feature_names, idf):\n",
    "            word_idf_dict[pair[0]] = pair[1]\n",
    "        \n",
    "        return feature_names, word_idf_dict\n",
    "\n",
    "    def get_probability_word_vectors(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: 分かち書き済みの文章のリスト\n",
    "        \"\"\"\n",
    "        \n",
    "        # GloVeの単語ベクトルを読み込む。\n",
    "        self.load_glove_vector()\n",
    "        \n",
    "        # 単語毎のGMMクラスタの確率ベクトル\n",
    "        idx, idx_proba = self.cluster_GMM()\n",
    " \n",
    "        # 各単語が属する確率が高いクラスタのインデックス\n",
    "        word_centroid_map = dict(zip(self.glove_vectors.index2word, idx))\n",
    "        # 各単語が、各クラスタに属する確率\n",
    "        word_centroid_prob_map = dict(zip(self.glove_vectors.index2word, idx_proba))     \n",
    "        \n",
    "        # TF-IDFを算出する。\n",
    "        featurenames, word_idf_dict = self.get_idf_dict(corpus)\n",
    "        \n",
    "        for word in word_centroid_map:\n",
    "            self.prob_wordvecs[word] = np.zeros(self.num_clusters * self.num_features, dtype=\"float32\")\n",
    "            for index in range(self.num_clusters):\n",
    "                try:\n",
    "                    self.prob_wordvecs[word][index*self.num_features:(index+1)*self.num_features] = \\\n",
    "                        self.glove_vectors[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "                except:\n",
    "                    continue\n",
    "        self.word_centroid_map = word_centroid_map\n",
    "\n",
    "    def create_cluster_vector_and_gwbowv(self, tokens, flag):\n",
    "        # SDV(Sparse Document Vector)を組み立てる。\n",
    "        \n",
    "        bag_of_centroids = np.zeros(self.num_clusters * self.num_features, dtype=\"float32\")\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                temp = self.word_centroid_map[token]\n",
    "            except:\n",
    "                continue\n",
    "            bag_of_centroids += self.prob_wordvecs[token]\n",
    "        norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "        if norm != 0:\n",
    "            bag_of_centroids /= norm\n",
    "            \n",
    "        # 訓練で作成したベクトルをスパース化するために最小と最大を記録しておく。\n",
    "        if flag:\n",
    "            self.min_no += min(bag_of_centroids)\n",
    "            self.max_no += max(bag_of_centroids)\n",
    "        return bag_of_centroids\n",
    "\n",
    "    def make_gwbowv(self, corpus, train=True):\n",
    "        # ドキュメントベクトルのマトリクスを作成する。\n",
    "        # gwbowvには通常のドキュメントベクトルが格納される。\n",
    "        gwbowv = np.zeros((len(corpus), self.num_clusters*self.num_features)).astype(np.float32)\n",
    "        cnt = 0\n",
    "        for tokens in tqdm(corpus):\n",
    "            gwbowv[cnt] = self.create_cluster_vector_and_gwbowv(tokens, train)\n",
    "            cnt += 1\n",
    "\n",
    "        return gwbowv\n",
    "\n",
    "    def dump_gwbowv(self, gwbowv, path=\"gwbowv_matrix.npy\", percentage=0.04):\n",
    "        # スパース化したドキュメントベクトルを保存する。\n",
    "        \n",
    "        # スパース化するための閾値を算出する。\n",
    "        min_no = self.min_no*1.0/gwbowv.shape[0]\n",
    "        max_no = self.max_no*1.0/gwbowv.shape[0]\n",
    "        print(\"Average min: \", min_no)\n",
    "        print(\"Average max: \", max_no)\n",
    "        thres = (abs(max_no) + abs(min_no))/2\n",
    "        thres = thres * percentage\n",
    "        \n",
    "        # 閾値未満のベクトルを0とし、スパース化する。\n",
    "        temp = abs(gwbowv) < thres\n",
    "        gwbowv[temp] = 0\n",
    "        np.save(path, gwbowv)\n",
    "        print(\"SDV created and dumped...\")\n",
    "\n",
    "    def load_matrix(self, name):\n",
    "        return np.load(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sklearn.svm import SVC\n",
    "from scdv import SparseCompositeDocumentVectors\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"GloVeとSCDVのパラメータの設定\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_clusters', type=int, default=20\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pname1', type=str, default=\"gmm_cluster.pkl\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pname2', type=str, default=\"gmm_prob_cluster.pkl\"\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main(args):\n",
    "    df = pd.read_csv('../elasticsearch/es_wakati.csv')\n",
    "    categories = df['業種(大分類)'].unique()\n",
    "    NUM_TOPICS = len(categories)\n",
    "\n",
    "    # 訓練データとtestデータに分ける\n",
    "    train_data, test_data, train_label, test_label, train_id, test_id = train_test_split(\n",
    "        df['分かち書き'], df['業種(大分類)'], df['ID'],\n",
    "        test_size=0.1, train_size=0.9, stratify=df['業種(大分類)'], shuffle=True)\n",
    "    \n",
    "    vec = SparseCompositeDocumentVectors(num_clusters, pname1, pname2)\n",
    "    # 確率重み付き単語ベクトルを求める\n",
    "    vec.get_probability_word_vectors(train_data)\n",
    "    # 訓練データからSCDVを求める\n",
    "    train_gwbowv = vec.make_gwbowv(train_data)\n",
    "    # テストデータからSCDVを求める\n",
    "    test_gwbowv = vec.make_gwbowv(test_data, False)\n",
    "\n",
    "    print(\"train size:{}  vector size:{}\".format(len(train_gwbowv), len(train_gwbowv[0])))\n",
    "    print(\"test size:{}  vector size:{}\".format(len(test_gwbowv), len(test_gwbowv[0])))\n",
    "\n",
    "    print(\"Test start...\")\n",
    "\n",
    "    start = time.time()\n",
    "    clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "    clf.fit(train_gwbowv, train_label)\n",
    "    test_pred = clf.predict(test_gwbowv)\n",
    "\n",
    "    # print(test_pred)\n",
    "\n",
    "    print (\"Report\")\n",
    "    print (classification_report(test_label, test_pred, digits=6))\n",
    "    print (\"Accuracy: \",clf.score(test_gwbowv, test_label))\n",
    "    print (\"Time taken:\", time.time() - start, \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Done...\n",
      "Cluster Assignments Saved...\n",
      "Probabilities of Cluster Assignments saved...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39429/39429 [00:07<00:00, 5050.83it/s]\n",
      "100%|██████████| 4381/4381 [00:00<00:00, 13191.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:39429  vector size:1000\n",
      "test size:4381  vector size:1000\n",
      "Test start...\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      その他の事業   0.510204  0.204918  0.292398       244\n",
      "      保健・衛生業   0.000000  0.000000  0.000000         3\n",
      "        保健衛生   0.000000  0.000000  0.000000         1\n",
      "       保健衛生業   0.065217  0.130435  0.086957        23\n",
      "          商業   0.500000  0.357320  0.416787       403\n",
      "         官公署   0.027027  0.500000  0.051282         2\n",
      "         建設業   0.692351  0.753799  0.721769      1645\n",
      "       接客娯楽業   0.173913  0.117647  0.140351        68\n",
      "      教育・研究業   0.000000  0.000000  0.000000         4\n",
      "       教育研究業   0.014493  0.066667  0.023810        15\n",
      "      映画・演劇業   0.000000  0.000000  0.000000         4\n",
      "      清掃・と畜業   0.440000  0.221477  0.294643       149\n",
      "      畜産・水産業   0.310345  0.465517  0.372414        58\n",
      "         製造業   0.593472  0.514801  0.551344       777\n",
      "       貨物取扱業   0.300000  0.254237  0.275229        59\n",
      "         農林業   0.600000  0.559585  0.579088       193\n",
      "         通信業   0.062500  0.176471  0.092308        17\n",
      "       運輸交通業   0.589587  0.612083  0.600624       629\n",
      "      金融・広告業   0.100000  0.218750  0.137255        32\n",
      "          鉱業   0.272727  0.272727  0.272727        55\n",
      "\n",
      "    accuracy                       0.556950      4381\n",
      "   macro avg   0.262592  0.271322  0.245449      4381\n",
      "weighted avg   0.581329  0.556950  0.561351      4381\n",
      "\n",
      "Accuracy:  0.5569504679296964\n",
      "Time taken: 101.46893405914307 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "num_clusters = 20\n",
    "pname1 = \"gmm_cluster.pkl\"\n",
    "pname2 = \"gmm_prob_cluster.pkl\"\n",
    "\n",
    "df = pd.read_csv('../elasticsearch/es_wakati.csv')\n",
    "# df = pd.read_csv('wakati_category_all.csv')\n",
    "categories = df['業種(大分類)'].unique()\n",
    "NUM_TOPICS = len(categories)\n",
    "\n",
    "# print(df.groupby(['業種(大分類)']).size())\n",
    "\n",
    "# 訓練データとtestデータに分ける\n",
    "train_data, test_data, train_label, test_label, train_id, test_id = train_test_split(\n",
    "    df['分かち書き'], df['業種(大分類)'], df['ID'],\n",
    "    test_size=0.1, train_size=0.9, stratify=df['業種(大分類)'], shuffle=True)\n",
    "\n",
    "'''\n",
    "train_id = train_id.values\n",
    "train_data = train_data.values\n",
    "train_label = train_label.values\n",
    "test_id = test_id.values\n",
    "test_data = test_data.values\n",
    "test_label = test_label.values\n",
    "'''\n",
    "\n",
    "vec = SparseCompositeDocumentVectors(num_clusters, pname1, pname2)\n",
    "# 確率重み付き単語ベクトルを求める\n",
    "vec.get_probability_word_vectors(train_data)\n",
    "# 訓練データからSCDVを求める\n",
    "train_gwbowv = vec.make_gwbowv(train_data)\n",
    "# テストデータからSCDVを求める\n",
    "test_gwbowv = vec.make_gwbowv(test_data, False)\n",
    "\n",
    "print(\"train size:{}  vector size:{}\".format(len(train_gwbowv), len(train_gwbowv[0])))\n",
    "print(\"test size:{}  vector size:{}\".format(len(test_gwbowv), len(test_gwbowv[0])))\n",
    "\n",
    "print(\"Test start...\")\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(train_gwbowv, train_label)\n",
    "test_pred = clf.predict(test_gwbowv)\n",
    "\n",
    "# print(test_pred)\n",
    "\n",
    "print (\"Report\")\n",
    "print (classification_report(test_label, test_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(test_gwbowv, test_label))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "num_clusters = 20\n",
    "pname1 = \"gmm_cluster.pkl\"\n",
    "pname2 = \"gmm_prob_cluster.pkl\"\n",
    "\n",
    "# df = pd.read_csv('../elasticsearch/es_wakati.csv')\n",
    "df = pd.read_csv('wakati_category_all.csv')\n",
    "categories = df['業種(大分類)'].unique()\n",
    "NUM_TOPICS = len(categories)\n",
    "\n",
    "print(df.groupby(['業種(大分類)']).size())\n",
    "\n",
    "all_data = df['分かち書き'].values\n",
    "\n",
    "\n",
    "vec = SparseCompositeDocumentVectors(num_clusters, pname1, pname2)\n",
    "# 確率重み付き単語ベクトルを求める\n",
    "vec.get_probability_word_vectors(all_data)\n",
    "# 訓練データからSCDVを求める\n",
    "gwbowv = vec.make_gwbowv(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gwbowv[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mhlw)",
   "language": "python",
   "name": "conda_mhlw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
