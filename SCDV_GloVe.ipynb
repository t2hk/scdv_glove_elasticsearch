{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SparseCompositeDocumentVectors:\n",
    "    def __init__(self, glove_word_vector_file, num_clusters,  pname1, pname2):\n",
    "        self.min_no = 0\n",
    "        self.max_no = 0\n",
    "        self.prob_wordvecs = {}\n",
    "        \n",
    "        #### 読み込むファイルの設定\n",
    "        # GloVeの単語ベクトルファイル\n",
    "        self.glove_word_vector_file = glove_word_vector_file\n",
    "\n",
    "        #### 出力するファイルの設定\n",
    "        # GloVeの単語ベクトルに単語数とベクトルサイズを付与したファイル\n",
    "        self.gensim_glove_word_vector_file = \"es_gensim_glove_vectors.txt\"\n",
    "        \n",
    "        # GMMの結果を保存するPickleファイル\n",
    "        self.pname1 = pname1\n",
    "        self.pname2 = pname2\n",
    "\n",
    "        #### その他パラメータ\n",
    "        # GMMのクラスタ数\n",
    "        self.num_clusters = num_clusters\n",
    "        \n",
    "        # GloVeの次元数\n",
    "        self.num_features = 50\n",
    "    \n",
    "    def load_glove_vector(self):\n",
    "        # GloVeの単語ベクトルファイルを読み込み、単語数とベクトルサイズを付与した処理用のファイルを作成する。\n",
    "        vectors = pd.read_csv(self.glove_word_vector_file, delimiter=' ', index_col=0, header=None)\n",
    "        \n",
    "        vocab_count = vectors.shape[0]  # 単語数\n",
    "        self.num_features = vectors.shape[1]  # 次元数\n",
    "\n",
    "        with open(self.glove_word_vector_file, 'r') as original, open(self.gensim_glove_word_vector_file, 'w') as transformed:\n",
    "            transformed.write(f'{vocab_count} {self.num_features}\\n')\n",
    "            transformed.write(original.read())  # 2行目以降はそのまま出力\n",
    "\n",
    "        # GloVeの単語ベクトルを読み込む        \n",
    "        self.glove_vectors = KeyedVectors.load_word2vec_format(self.gensim_glove_word_vector_file, binary=False)\n",
    "        \n",
    "    def cluster_GMM2(self):   \n",
    "        glove_vectors = self.glove_vectors.vectors\n",
    "        \n",
    "        # Initalize a GMM object and use it for clustering.\n",
    "        gmm_model = GaussianMixture(n_components=num_clusters, covariance_type=\"tied\", init_params='kmeans', max_iter=100)\n",
    "        # Get cluster assignments.\n",
    "        gmm_model.fit(glove_vectors)\n",
    "        idx = gmm_model.predict(glove_vectors)\n",
    "        print (\"Clustering Done...\")\n",
    "        # Get probabilities of cluster assignments.\n",
    "        idx_proba = gmm_model.predict_proba(glove_vectors)\n",
    "        # Dump cluster assignments and probability of cluster assignments. \n",
    "        pickle.dump(idx, open(self.pname1,\"wb\"))\n",
    "        print (\"Cluster Assignments Saved...\")\n",
    "\n",
    "        pickle.dump(idx_proba,open(self.pname2, \"wb\"))\n",
    "        print (\"Probabilities of Cluster Assignments Saved...\")\n",
    "        return (idx, idx_proba)        \n",
    "        \n",
    "    def cluster_GMM(self):\n",
    "        # GMMによるクラスタリング\n",
    "        \n",
    "        clf = GaussianMixture(\n",
    "            n_components=self.num_clusters,\n",
    "            #covariance_type=\"tied\",\n",
    "            covariance_type=\"diag\",            \n",
    "            init_params=\"kmeans\",\n",
    "            max_iter=50\n",
    "        )\n",
    "        \n",
    "        glove_vectors = self.glove_vectors.vectors\n",
    "        # Get cluster assignments.\n",
    "        clf.fit(glove_vectors)\n",
    "        idx = clf.predict(glove_vectors)\n",
    "        print(\"Clustering Done...\")\n",
    "        # Get probabilities of cluster assignments.\n",
    "        idx_proba = clf.predict_proba(glove_vectors)\n",
    "        # Dump cluster assignments and probability of cluster assignments.\n",
    "        pickle.dump(idx, open(self.pname1, \"wb\"))\n",
    "        print(\"Cluster Assignments Saved...\")\n",
    "        pickle.dump(idx_proba, open(self.pname2, \"wb\"))\n",
    "        print(\"Probabilities of Cluster Assignments saved...\")\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    def read_GMM(self):\n",
    "        # GMMモデルを読み込む。\n",
    "        \n",
    "        idx = pickle.load(open(self.idx_name, \"rb\"))\n",
    "        idx_proba = pickle.load(open(self.idx_proba_name, \"rb\"))\n",
    "        print(\"Cluster Model Loaded...\")\n",
    "        return (idx, idx_proba)\n",
    "\n",
    "    def get_idf_dict(self, corpus):\n",
    "        # IDFを算出する。\n",
    "        # corpus : 分かち書きした文章のリスト\n",
    "        \n",
    "        # 単語の数をカウントする\n",
    "        count_vectorizer = CountVectorizer()\n",
    "        X_count = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # scikit-learn の TF-IDF 実装\n",
    "        tfidf_vectorizer = TfidfVectorizer(token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        feature_names = tfidf_vectorizer.get_feature_names()\n",
    "        idf = tfidf_vectorizer.idf_\n",
    "\n",
    "        word_idf_dict = {}\n",
    "        for pair in zip(feature_names, idf):\n",
    "            word_idf_dict[pair[0]] = pair[1]\n",
    "        \n",
    "        return feature_names, word_idf_dict\n",
    "\n",
    "    def get_probability_word_vectors(self, corpus):\n",
    "        \"\"\"\n",
    "        corpus: 分かち書き済みの文章のリスト\n",
    "        \"\"\"\n",
    "        \n",
    "        # GloVeの単語ベクトルを読み込む。\n",
    "        self.load_glove_vector()\n",
    "        \n",
    "        # 単語毎のGMMクラスタの確率ベクトル\n",
    "        idx, idx_proba = self.cluster_GMM()\n",
    " \n",
    "        # 各単語が属する確率が高いクラスタのインデックス\n",
    "        word_centroid_map = dict(zip(self.glove_vectors.index2word, idx))\n",
    "        # 各単語が、各クラスタに属する確率\n",
    "        word_centroid_prob_map = dict(zip(self.glove_vectors.index2word, idx_proba))     \n",
    "        \n",
    "        # TF-IDFを算出する。\n",
    "        featurenames, word_idf_dict = self.get_idf_dict(corpus)\n",
    "        \n",
    "        for word in word_centroid_map:\n",
    "            self.prob_wordvecs[word] = np.zeros(self.num_clusters * self.num_features, dtype=\"float32\")\n",
    "            for index in range(self.num_clusters):\n",
    "                try:\n",
    "                    self.prob_wordvecs[word][index*self.num_features:(index+1)*self.num_features] = \\\n",
    "                        self.glove_vectors[word] * word_centroid_prob_map[word][index] * word_idf_dict[word]\n",
    "                except:\n",
    "                    continue\n",
    "        self.word_centroid_map = word_centroid_map\n",
    "\n",
    "    def create_cluster_vector_and_gwbowv(self, tokens, flag):\n",
    "        # SDV(Sparse Document Vector)を組み立てる。\n",
    "        \n",
    "        bag_of_centroids = np.zeros(self.num_clusters * self.num_features, dtype=\"float32\")\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                temp = self.word_centroid_map[token]\n",
    "            except:\n",
    "                continue\n",
    "            bag_of_centroids += self.prob_wordvecs[token]\n",
    "        norm = np.sqrt(np.einsum('...i,...i', bag_of_centroids, bag_of_centroids))\n",
    "        if norm != 0:\n",
    "            bag_of_centroids /= norm\n",
    "            \n",
    "        # 訓練で作成したベクトルをスパース化するために最小と最大を記録しておく。\n",
    "        if flag:\n",
    "            self.min_no += min(bag_of_centroids)\n",
    "            self.max_no += max(bag_of_centroids)\n",
    "        return bag_of_centroids\n",
    "\n",
    "    def make_gwbowv(self, corpus, train=True):\n",
    "        # ドキュメントベクトルのマトリクスを作成する。\n",
    "        # gwbowvには通常のドキュメントベクトルが格納される。\n",
    "        gwbowv = np.zeros((len(corpus), self.num_clusters*self.num_features)).astype(np.float32)\n",
    "        cnt = 0\n",
    "        for tokens in tqdm(corpus):\n",
    "            gwbowv[cnt] = self.create_cluster_vector_and_gwbowv(tokens, train)\n",
    "            cnt += 1\n",
    "\n",
    "        return gwbowv\n",
    "\n",
    "    def dump_gwbowv(self, gwbowv, path=\"gwbowv_matrix.npy\", percentage=0.04):\n",
    "        # スパース化したドキュメントベクトルを保存する。\n",
    "        \n",
    "        # スパース化するための閾値を算出する。\n",
    "        min_no = self.min_no*1.0/gwbowv.shape[0]\n",
    "        max_no = self.max_no*1.0/gwbowv.shape[0]\n",
    "        print(\"Average min: \", min_no)\n",
    "        print(\"Average max: \", max_no)\n",
    "        thres = (abs(max_no) + abs(min_no))/2\n",
    "        thres = thres * percentage\n",
    "        \n",
    "        # 閾値未満のベクトルを0とし、スパース化する。\n",
    "        temp = abs(gwbowv) < thres\n",
    "        gwbowv[temp] = 0\n",
    "        np.save(path, gwbowv)\n",
    "        print(\"SDV created and dumped...\")\n",
    "\n",
    "    def load_matrix(self, name):\n",
    "        return np.load(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from sklearn.svm import SVC\n",
    "from scdv import SparseCompositeDocumentVectors\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"GloVeとSCDVのパラメータの設定\"\n",
    "    )\n",
    "    parser.add_argument('--glove_word_vector_file', type=str)\n",
    "    parser.add_argument('--csv_file', type=str)\n",
    "    parser.add_argument(\n",
    "        '--num_clusters', type=int, default=20\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pname1', type=str, default=\"gmm_cluster.pkl\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pname2', type=str, default=\"gmm_prob_cluster.pkl\"\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main(args):\n",
    "    df = pd.read_csv(args.csv_file)\n",
    "    categories = df['業種(大分類)'].unique()\n",
    "    NUM_TOPICS = len(categories)\n",
    "\n",
    "    # 訓練データとtestデータに分ける\n",
    "    train_data, test_data, train_label, test_label, train_id, test_id = train_test_split(\n",
    "        df['分かち書き'], df['業種(大分類)'], df['ID'],\n",
    "        test_size=0.1, train_size=0.9, stratify=df['業種(大分類)'], shuffle=True)\n",
    "    \n",
    "    vec = SparseCompositeDocumentVectors(args.glove_word_vector_file, args.num_clusters, args.pname1, args.pname2)\n",
    "    # 確率重み付き単語ベクトルを求める\n",
    "    vec.get_probability_word_vectors(train_data)\n",
    "    # 訓練データからSCDVを求める\n",
    "    train_gwbowv = vec.make_gwbowv(train_data)\n",
    "    # テストデータからSCDVを求める\n",
    "    test_gwbowv = vec.make_gwbowv(test_data, False)\n",
    "\n",
    "    print(\"train size:{}  vector size:{}\".format(len(train_gwbowv), len(train_gwbowv[0])))\n",
    "    print(\"test size:{}  vector size:{}\".format(len(test_gwbowv), len(test_gwbowv[0])))\n",
    "\n",
    "    print(\"Test start...\")\n",
    "\n",
    "    start = time.time()\n",
    "    clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "    clf.fit(train_gwbowv, train_label)\n",
    "    test_pred = clf.predict(test_gwbowv)\n",
    "\n",
    "    # print(test_pred)\n",
    "\n",
    "    print (\"Report\")\n",
    "    print (classification_report(test_label, test_pred, digits=6))\n",
    "    print (\"Accuracy: \",clf.score(test_gwbowv, test_label))\n",
    "    print (\"Time taken:\", time.time() - start, \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "num_clusters = 20\n",
    "pname1 = \"gmm_cluster.pkl\"\n",
    "pname2 = \"gmm_prob_cluster.pkl\"\n",
    "glove_word_vector_file = \"glove_word_vector_file.txt\"\n",
    "\n",
    "df = pd.read_csv('../elasticsearch/es_wakati.csv')\n",
    "# df = pd.read_csv('wakati_category_all.csv')\n",
    "categories = df['業種(大分類)'].unique()\n",
    "NUM_TOPICS = len(categories)\n",
    "\n",
    "# print(df.groupby(['業種(大分類)']).size())\n",
    "\n",
    "# 訓練データとtestデータに分ける\n",
    "train_data, test_data, train_label, test_label, train_id, test_id = train_test_split(\n",
    "    df['分かち書き'], df['業種(大分類)'], df['ID'],\n",
    "    test_size=0.1, train_size=0.9, stratify=df['業種(大分類)'], shuffle=True)\n",
    "\n",
    "'''\n",
    "train_id = train_id.values\n",
    "train_data = train_data.values\n",
    "train_label = train_label.values\n",
    "test_id = test_id.values\n",
    "test_data = test_data.values\n",
    "test_label = test_label.values\n",
    "'''\n",
    "\n",
    "vec = SparseCompositeDocumentVectors(glove_word_vector_file, num_clusters, pname1, pname2)\n",
    "# 確率重み付き単語ベクトルを求める\n",
    "vec.get_probability_word_vectors(train_data)\n",
    "# 訓練データからSCDVを求める\n",
    "train_gwbowv = vec.make_gwbowv(train_data)\n",
    "# テストデータからSCDVを求める\n",
    "test_gwbowv = vec.make_gwbowv(test_data, False)\n",
    "\n",
    "print(\"train size:{}  vector size:{}\".format(len(train_gwbowv), len(train_gwbowv[0])))\n",
    "print(\"test size:{}  vector size:{}\".format(len(test_gwbowv), len(test_gwbowv[0])))\n",
    "\n",
    "print(\"Test start...\")\n",
    "\n",
    "start = time.time()\n",
    "clf = lgb.LGBMClassifier(objective=\"multiclass\")\n",
    "clf.fit(train_gwbowv, train_label)\n",
    "test_pred = clf.predict(test_gwbowv)\n",
    "\n",
    "# print(test_pred)\n",
    "\n",
    "print (\"Report\")\n",
    "print (classification_report(test_label, test_pred, digits=6))\n",
    "print (\"Accuracy: \",clf.score(test_gwbowv, test_label))\n",
    "print (\"Time taken:\", time.time() - start, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "\n",
    "num_clusters = 20\n",
    "pname1 = \"gmm_cluster.pkl\"\n",
    "pname2 = \"gmm_prob_cluster.pkl\"\n",
    "\n",
    "df = pd.read_csv('wakati_category_all.csv')\n",
    "categories = df['業種(大分類)'].unique()\n",
    "NUM_TOPICS = len(categories)\n",
    "\n",
    "print(df.groupby(['業種(大分類)']).size())\n",
    "\n",
    "all_data = df['分かち書き'].values\n",
    "\n",
    "\n",
    "vec = SparseCompositeDocumentVectors(num_clusters, pname1, pname2)\n",
    "# 確率重み付き単語ベクトルを求める\n",
    "vec.get_probability_word_vectors(all_data)\n",
    "# 訓練データからSCDVを求める\n",
    "gwbowv = vec.make_gwbowv(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gwbowv[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mhlw)",
   "language": "python",
   "name": "conda_mhlw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
